# -*- coding: utf-8 -*-
"""Diabetic Transfer model with predictionipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-RV2iYcY-5Sqtlf257GMveDfL1-9BmsS
"""

from google.colab import drive
drive.mount('/content/drive/')

import os
os.chdir('/content/drive/MyDrive/Datasets/Eyes dataset preprocessed with labels')
print("We are currently in the folder of ",os.getcwd())

import pickle
def load_data():
    file = open('df_train_train', 'rb')
    df_train_train = pickle.load(file)
    file.close()

    file = open('df_test', 'rb')
    df_test = pickle.load(file)
    file.close()
    
    return df_train_train, df_test

df_train_train,df_test = load_data()
print(df_train_train.shape,df_test.shape,'\n')

df_train_train.head()

df_test.head()

# Model parameters
IMG_SIZE = 512
BATCH_SIZE = 8
#EPOCHS = 40
WARMUP_EPOCHS = 2
LEARNING_RATE = 1e-4
WARMUP_LEARNING_RATE = 1e-3
HEIGHT = 320
WIDTH = 320
CANAL = 3
N_CLASSES = df_train_train['diagnosis'].nunique()
ES_PATIENCE = 5
RLROP_PATIENCE = 3
DECAY_DROP = 0.5

from keras.preprocessing.image import ImageDataGenerator

train_datagen= ImageDataGenerator(rescale=1./255, validation_split=0.2, horizontal_flip=True)
 
test_datagen = ImageDataGenerator(rescale=1./255)

train_generator= train_datagen.flow_from_dataframe(dataframe= df_train_train,
                                                     directory= "/content/drive/MyDrive/Datasets/Eyes dataset preprocessed/train_images_resized_preprocessed",
                                                     x_col= "file_name",
                                                     y_col= "diagnosis",
                                                     batch_size= BATCH_SIZE,
                                                     class_mode= "categorical",
                                                     target_size= (HEIGHT,WIDTH),
                                                     subset= 'training'
                                                     )

valid_generator= train_datagen.flow_from_dataframe(dataframe=df_train_train,
                                                      directory="./train_images_resized_preprocessed/",
                                                      x_col="file_name",
                                                      y_col="diagnosis",
                                                      batch_size=BATCH_SIZE,
                                                      class_mode="categorical",    
                                                      target_size=(HEIGHT, WIDTH),
                                                      subset='validation')

test_generator = test_datagen.flow_from_dataframe(dataframe= df_test,
                                                    directory= "./test_images_resized_preprocessed/",
                                                    x_col= "file_name", 
                                                    target_size= (HEIGHT, WIDTH),
                                                    batch_size=1,
                                                    shuffle=False,
                                                    class_mode= None
                                                    )

"""Model Building """

import pandas as pd
import numpy as np
import os
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.metrics import confusion_matrix, cohen_kappa_score,accuracy_score

from PIL import Image
import cv2

import keras
from keras import applications
from keras.preprocessing.image import ImageDataGenerator
from keras import optimizers,Model,Sequential
from keras.layers import Input,GlobalAveragePooling2D,Dropout,Dense,Activation
from keras.callbacks import EarlyStopping,ReduceLROnPlateau

from tensorflow.keras.applications import ResNet50

from keras.utils.traceback_utils import include_frame
def create_model(input_shape, n_out):
  input_tensor= Input(shape= input_shape)
  base_model = ResNet50(weights= None, include_top= False, input_tensor= input_tensor)
  base_model.load_weights('resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5')

  x= GlobalAveragePooling2D()(base_model.output)  #It will return the average of feature value instead of sliding over it. 
  x= Dropout(0.5)(x)
  x= Dense(2048, activation='relu')(x)
  x= Dropout(0.5)(x)

  final_output= Dense(n_out, activation='softmax', name= 'final_output')(x)
  model= Model(input_tensor, final_output)

  return model

# assigning parameters

model= create_model(input_shape=(HEIGHT, WIDTH, CANAL), n_out= N_CLASSES)

#Turn off weights 

for layer in model.layers:
  layer.trainable= False

#train 5 layers only 

for i in range(-5,0):
  model.layers[i].trainable= True

model.summary()

STEP_SIZE_TRAIN = train_generator.n//train_generator.batch_size

print(STEP_SIZE_TRAIN)

STEP_SIZE_VALID = valid_generator.n//valid_generator.batch_size
print(STEP_SIZE_VALID)

#Compile the model 
from sklearn import metrics
from tensorflow.keras.optimizers import Adam
model.compile(optimizer= Adam(learning_rate= WARMUP_LEARNING_RATE), loss= 'categorical_crossentropy', metrics= ['accuracy'])

#Fit into the model 

history_warmup = model.fit(train_generator,
                           steps_per_epoch= STEP_SIZE_TRAIN,
                           validation_data= valid_generator,
                           validation_steps=STEP_SIZE_VALID, 
                           epochs= WARMUP_EPOCHS,
                           verbose=1
                           ).history

# Training all the layers 

for layer in model.layers:
  layer.trainable= True

Erl= EarlyStopping(monitor= 'val_loss', mode= 'min', patience=ES_PATIENCE, restore_best_weights=True, verbose=1)
rlrop= ReduceLROnPlateau(monitor= 'val_loss', mode= 'min', patience=RLROP_PATIENCE, factor=DECAY_DROP, min_lr=1e-6, verbose=1)
#rlrop will reduce the learning rate by DECAY_DROP factor

callback_list= [Erl, rlrop]

optimizer= Adam(learning_rate= LEARNING_RATE)
model.compile(optimizer= optimizer, loss= "binary_crossentropy", metrics= ['accuracy'])
model.summary()

history_finetunning= model.fit(train_generator, 
                               steps_per_epoch= STEP_SIZE_TRAIN,
                               validation_data=valid_generator,
                               validation_steps=STEP_SIZE_VALID,
                               epochs= 40,
                               callbacks= callback_list,
                               verbose=1
                               ).history

plt.figure(figsize=(8,5))

plt.plot(history_finetunning['accuracy'])
plt.plot(history_finetunning['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.gca().ticklabel_format(axis='both', style='plain', useOffset=False)
plt.show()

#Save the model 

model.save('Diabetic_model.h5')

import os
os.chdir('/content/drive/MyDrive/Datasets/Eyes dataset preprocessed with labels')
print("We are currently in the folder of ",os.getcwd())

"""Loading the model """

from keras.models import load_model
model = load_model('/content/drive/MyDrive/Datasets/Eyes dataset preprocessed with labels/Diabetic_model.h5')

"""Generating Train Prediction on Train data """

complete_datagen = ImageDataGenerator(rescale=1./255)

complete_generator = complete_datagen.flow_from_dataframe(dataframe=df_train_train,
                                                          directory="./train_images_resized_preprocessed/",
                                                          x_col="file_name",
                                                          target_size= (HEIGHT,WIDTH),
                                                          batch_size=1,
                                                          shuffle= False,
                                                          class_mode=None
                                                          )

STEP_SIZE_COMPLETE= complete_generator.n//complete_generator.batch_size
train_preds = model.predict(complete_generator, steps= STEP_SIZE_COMPLETE, verbose=1)
train_preds= [np.argmax(pred) for pred in train_preds]
print('\n Train Prediction completed', '\n')

"""checking on test data"""

test_generator.reset()
STEP_SIZE_TEST = test_generator.n//test_generator.batch_size
test_preds = model.predict(test_generator, steps=STEP_SIZE_TEST,verbose = 1)
test_labels = [np.argmax(pred) for pred in test_preds]

print("Test Accuracy score : %.4f" % accuracy_score(df_test['diagnosis'].astype('int'),test_labels))

# Accuracy on validation Data 
val_loss, val_accuracy = model.evaluate(valid_generator)

# Printing the validation results
print('Val loss: {0:.4f}. Val accuracy: {1:.2f}%'.format(val_loss, val_accuracy*100.))

def plot_conf_matrix(true, pred, classes):
  cf = confusion_matrix(true, pred)

  df_cm = pd.DataFrame(cf, range(len(classes)), range(len(classes)))
  plt.figure(figsize=(10,6))
  sns.set(font_scale=1.4)
  sns.heatmap(df_cm, annot= True, annot_kws={'size':16}, xticklabels= classes, yticklabels= classes, fmt='g')
  plt.show()

labels = ['0 - No DR', '1 - Mild', '2 - Moderate', '3 - Severe', '4 - Proliferative DR']
plot_conf_matrix(list(df_test['diagnosis'].astype(int)),test_labels,labels)

import keras
import cv2
from keras.models import load_model, Model
import numpy as np
from keras.preprocessing import image

file_path1= '/content/drive/MyDrive/Datasets/Eyes dataset preprocessed with labels/test_images_resized_preprocessed/e4dcca36ceb4.png' #class0
#file_path2= '/content/drive/MyDrive/Datasets/Eyes dataset preprocessed/test_images_resized_preprocessed/e4e343eaae2a.png' #class2
#file_path3= '/content/drive/MyDrive/Datasets/Eyes dataset preprocessed/test_images_resized_preprocessed/ff8a0b45c789.png' #class4

img = cv2.imread(file_path1)

import cv2
imgs = cv2.resize(img, (320,320))

imgs=image.img_to_array(imgs)
imgs=imgs/255

imgs=np.expand_dims(imgs,axis=0)

imgs.shape

result = model.predict(imgs)

result

classes1= ['0 - No DR', '1 - Mild', '2 - Moderate', '3 - Severe', '4 - Proliferative DR']
num_classes = len(classes1)
print (classes1)

sorted_prob_idxs = (-result).argsort()[0]

predicted_prob = np.amax(result)

predicted_probs= []
predicted_probs.append(predicted_prob)

predicted_class = classes1[sorted_prob_idxs[0]]

predicted_classes=[]
predicted_classes.append(predicted_class)

print (result, '\n')
print("Class Type: ", predicted_class)

labels = ['0 - No DR', '1 - Mild', '2 - Moderate', '3 - Severe', '4 - Proliferative DR']

'''
0 - No DR

1 - Mild

2 - Moderate

3 - Severe

4 - Proliferative DR

'''

